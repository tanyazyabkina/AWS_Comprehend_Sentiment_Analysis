{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Comprehend Sentiment Analysis Using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to use boto3 Amazon API to use Amazon Comprehend for real time analysis as well as scheduling analysis jobs.\n",
    "1. For boto3 to work you need to create an IAM User, receive `aws_access_key_id` and `aws_secret_access_key` and configure your credentials using AWS Command Line Interface (AWS CLI)\n",
    "2. Cost. If you are using free AWS tier, you can analyze 50K units a month free.  Every unit is 100 characters. In my example, every tweet is ~2 units. In the scheduled job I am analyzing 10K tweets at once, so the free tier runs out pretty fast, and then it's \\$1 per 10K. Be sure to check pricing before you proceed. https://aws.amazon.com/comprehend/pricing/\n",
    "3. Reference. Boto3 S3: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html  Boto3 Comprehend: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import pandas as pd\n",
    "import json\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_to_data = \"https://github.com/tanyazyabkina/AmazonComprehend/blob/master/walmart_tweets_1k.csv?raw=true\"\n",
    "\n",
    "local_file_name = 'Comprehend/walmart_1k.csv'\n",
    "\n",
    "df = pd.read_csv(link_to_data, header = None, names = ['walmart_tweets'], dtype = 'str', encoding = 'utf-8') \n",
    "df.to_csv(local_file_name, encoding = 'utf-8', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>walmart_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tony Hawkâ€™s Pro Skater 1+2 (PS4) is $33.88 on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@CassieFambro we were just saying that yesterd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@lxoG21 I love me some Walmart candles lol the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I actually am too ðŸ¤” need to go shopping. 24/7 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@diancalondon Bill was.....Sunday morning Khak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      walmart_tweets\n",
       "0  Tony Hawkâ€™s Pro Skater 1+2 (PS4) is $33.88 on ...\n",
       "1  @CassieFambro we were just saying that yesterd...\n",
       "2  @lxoG21 I love me some Walmart candles lol the...\n",
       "3  I actually am too ðŸ¤” need to go shopping. 24/7 ...\n",
       "4  @diancalondon Bill was.....Sunday morning Khak..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Single Record Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this type of processing you can analyze one piece of text of up to 5K bytes long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I actually am too ðŸ¤” need to go shopping. 24/7 Walmart come back pls https://t.co/BB5cXNbSzo\n"
     ]
    }
   ],
   "source": [
    "# Record to examine\n",
    "text = df.loc[3].item()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize Comprehend module\n",
    "comprehend = boto3.client(service_name='comprehend', region_name='us-east-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sentiment': 'NEUTRAL',\n",
       " 'SentimentScore': {'Positive': 0.08862289786338806,\n",
       "  'Negative': 0.09810954332351685,\n",
       "  'Neutral': 0.8021741509437561,\n",
       "  'Mixed': 0.011093364097177982},\n",
       " 'ResponseMetadata': {'RequestId': '1704d62a-20c6-4782-b3c8-ae51a8436116',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1704d62a-20c6-4782-b3c8-ae51a8436116',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '162',\n",
       "   'date': 'Fri, 30 Apr 2021 20:28:06 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run sentiment analysis\n",
    "sentiment_output = comprehend.detect_sentiment(Text=text, LanguageCode='en')\n",
    "# Output\n",
    "sentiment_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Positive': 0.08862289786338806,\n",
       " 'Negative': 0.09810954332351685,\n",
       " 'Neutral': 0.8021741509437561,\n",
       " 'Mixed': 0.011093364097177982}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scores\n",
    "sentiment_output['SentimentScore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEUTRAL'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment\n",
    "sentiment_output['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Batch Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to 25 documents of up to 5,000 bytes each, submitted in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch\n",
    "text_list = list(df.walmart_tweets[0:25])\n",
    "\n",
    "#Run a sentiment batch\n",
    "sentiment_batch = comprehend.batch_detect_sentiment(TextList=text_list,\n",
    "                                                    LanguageCode='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single record check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@diancalondon Bill was.....Sunday morning Khaki Walmart fly....in his own way...the heart wants what it wants. Yeah. Maybe the pickens are slim midwest? The only one I understood and felt bad for was Barb, because she felt she owed Bill for being there. https://t.co/BOCvIDvAmc'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Index': 4,\n",
       " 'Sentiment': 'NEUTRAL',\n",
       " 'SentimentScore': {'Positive': 0.07533818483352661,\n",
       "  'Negative': 0.30534008145332336,\n",
       "  'Neutral': 0.6191905736923218,\n",
       "  'Mixed': 0.0001311323867412284}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_batch['ResultList'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the results into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentiment_batch(data):\n",
    "    df = pd.DataFrame([item['SentimentScore'] for item in data['ResultList']])\n",
    "    df['Sentiment'] = [item.get('Sentiment') for item in data['ResultList']]\n",
    "    df['Index'] = [item.get('Index') for item in data['ResultList']]\n",
    "    df.set_index('Index', inplace = True)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Mixed</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027284</td>\n",
       "      <td>0.519825</td>\n",
       "      <td>0.452641</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.995531</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.088623</td>\n",
       "      <td>0.098110</td>\n",
       "      <td>0.802174</td>\n",
       "      <td>0.011093</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.075338</td>\n",
       "      <td>0.305340</td>\n",
       "      <td>0.619191</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Positive  Negative   Neutral     Mixed Sentiment\n",
       "Index                                                  \n",
       "0      0.000804  0.000075  0.999100  0.000022   NEUTRAL\n",
       "1      0.027284  0.519825  0.452641  0.000250  NEGATIVE\n",
       "2      0.995531  0.000094  0.004349  0.000026  POSITIVE\n",
       "3      0.088623  0.098110  0.802174  0.011093   NEUTRAL\n",
       "4      0.075338  0.305340  0.619191  0.000131   NEUTRAL"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_sentiment_batch(sentiment_batch).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Processing - Scheduling an Analysis Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I highly recommend that you run at least one Comprehend job from the point and click interface, especially, if you are new to AWS. This way you can create a data access role (aka `data_access_role_arn`), and then you can simply copy the role name from the job description. \n",
    "\n",
    "You will need to create your S3 bucket through the web interface or through BOTO3 API.\n",
    "\n",
    "Note that I use different folders `s3://comprehend-api/input-data` and `s3://comprehend-api/results` for input data and results output. This way, your results are not going to get confused for inputs if you were to analyze all files in the folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file_name = 'Comprehend/walmart_1k.csv'\n",
    "bucket_name  = 'comprehend-api'\n",
    "aws_file_name = 'input-data/walmart_1k.csv'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Upload file to specific location\n",
    "s3.upload_file(local_file_name, bucket_name, aws_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Sentiment Detection Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these values before running the program\n",
    "input_s3_url = 's3://comprehend-api/input-data/walmart_1k.csv' #folder, file or prefix\n",
    "input_doc_format = 'ONE_DOC_PER_LINE'\n",
    "output_s3_url = 's3://comprehend-api/results'\n",
    "data_access_role_arn = \"arn:aws:iam::YOUR_ACCOUNT_NUMBER:role/service-role/YOUR_SERVICE_ROLE\"\n",
    "\n",
    "# Set up job configuration\n",
    "input_data_config = {'S3Uri': input_s3_url, 'InputFormat': input_doc_format}\n",
    "output_data_config = {'S3Uri': output_s3_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Sentiment Detection JobID is: b6a7626c25c4c5ffcf558901743eb8ff\n"
     ]
    }
   ],
   "source": [
    "# Start the client\n",
    "comprehend = boto3.client('comprehend')\n",
    "\n",
    "# Begin a job to detect the topics in the document collection\n",
    "start_job_sentiment = comprehend.start_sentiment_detection_job(\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    DataAccessRoleArn=data_access_role_arn,\n",
    "    LanguageCode='en',\n",
    "    JobName='Walmart_1K_tweets')\n",
    "job_id = start_job_sentiment['JobId']\n",
    "print(f'Your Sentiment Detection JobID is: {job_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your job runs asynchronously. **This may take several minutes to run.**\n",
    "When the status turns to COMPLETED, you can retrieve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: IN_PROGRESS\n"
     ]
    }
   ],
   "source": [
    "# Retrieve information about the job - the job may take a while to run\n",
    "describe_result = comprehend.describe_sentiment_detection_job(JobId=job_id)\n",
    "job_status = describe_result['SentimentDetectionJobProperties']['JobStatus']\n",
    "print(f'Job Status: {job_status}')\n",
    "if job_status == 'FAILED':\n",
    "    print(f'Reason: {describe_result[\"SentimentDetectionJobProperties\"][\"Message\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The results are located here\n",
    "results_S3Url = comprehend.describe_sentiment_detection_job(\n",
    "    JobId=job_id)['SentimentDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "\n",
    "# Your Output S3 Url\n",
    "results_S3Url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't know where your results are, get a listing of your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files and folders in the bucket\n",
    "def s3_bucket_list_obj(bucket):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket)\n",
    "    for obj in bucket.objects.all():\n",
    "        print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files\n",
    "s3_bucket_list_obj(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give your local results file a name\n",
    "results_name = 'sentiment'\n",
    "\n",
    "local_results_filename = 'Comprehend/outputs/' + results_name + '.tar.gz'\n",
    "s3_name = 's3://' + bucket_name + '/'\n",
    "results_aws_filename = results_S3Url.replace(s3_name, '')\n",
    "\n",
    "# Download results\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(bucket_name,\n",
    "                 results_aws_filename, \n",
    "                 local_results_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the file\n",
    "def extract_targz(targz_file, output_path = ''):\n",
    "    if targz_file.endswith(\"tar.gz\"):\n",
    "        tar = tarfile.open(targz_file, \"r:gz\")\n",
    "        tar.extractall(path = output_path)\n",
    "        tar.close()\n",
    "    elif targz_file.endswith(\"tar\"):\n",
    "        tar = tarfile.open(targz_file, \"r:\")\n",
    "        tar.extractall(path = output_path)\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a temp file is called 'output'\n",
    "results_name = 'sentiment'\n",
    "local_results_filename = 'Comprehend/outputs/' + results_name + '.tar.gz'\n",
    "output_path = 'Comprehend/outputs/extracted' \n",
    "extract_targz(local_results_filename, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the output: 1000\n"
     ]
    }
   ],
   "source": [
    "# Read JSON into a dictionary   \n",
    "input_file = output_path + '/output'\n",
    "results = [json.loads(line) for line in open(input_file, 'r')]\n",
    "print('Number of records in the output:',len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the output looks like. Note that often the records are not in the same order that they were sent in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'File': 'walmart_1k.csv',\n",
       " 'Line': 2,\n",
       " 'Sentiment': 'POSITIVE',\n",
       " 'SentimentScore': {'Mixed': 2.5774566893232986e-05,\n",
       "  'Negative': 9.411451173946261e-05,\n",
       "  'Neutral': 0.00434883451089263,\n",
       "  'Positive': 0.9955312609672546}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function parses sentiment data into a dataframe\n",
    "def parse_sentiment(data):\n",
    "    df = pd.DataFrame([item['SentimentScore'] for item in data])\n",
    "    df['File'] = [item.get('File') for item in data]\n",
    "    df['Sentiment'] = [item.get('Sentiment') for item in data]\n",
    "    df['Line'] = [item.get('Line') for item in data]\n",
    "    df.set_index('Line', inplace = True)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the dataframe\n",
    "# Let's sort and index the dataframe by line\n",
    "sentiment_results = parse_sentiment(results).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mixed</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "      <th>File</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Line</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>walmart_1k.csv</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.542811</td>\n",
       "      <td>0.416986</td>\n",
       "      <td>0.040052</td>\n",
       "      <td>walmart_1k.csv</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.995531</td>\n",
       "      <td>walmart_1k.csv</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011093</td>\n",
       "      <td>0.098110</td>\n",
       "      <td>0.802174</td>\n",
       "      <td>0.088623</td>\n",
       "      <td>walmart_1k.csv</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.353730</td>\n",
       "      <td>0.539221</td>\n",
       "      <td>0.106831</td>\n",
       "      <td>walmart_1k.csv</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Mixed  Negative   Neutral  Positive            File Sentiment\n",
       "Line                                                                  \n",
       "0     0.000022  0.000075  0.999100  0.000804  walmart_1k.csv   NEUTRAL\n",
       "1     0.000151  0.542811  0.416986  0.040052  walmart_1k.csv  NEGATIVE\n",
       "2     0.000026  0.000094  0.004349  0.995531  walmart_1k.csv  POSITIVE\n",
       "3     0.011093  0.098110  0.802174  0.088623  walmart_1k.csv   NEUTRAL\n",
       "4     0.000217  0.353730  0.539221  0.106831  walmart_1k.csv   NEUTRAL"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a record and validate that the results were similar (they won't be exactly the same), and that we sorted the dataframe correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWEET TEXT:\n",
      " life was so much simpler when i would go to walmart with my mom and play plants vs zombies on all the display computers while she grocery shopped ðŸ˜”ðŸ˜”ðŸ˜”\n",
      "\n",
      "REAL TIME RESULTS:\n",
      "\n",
      "{'Positive': 0.5083320140838623, 'Negative': 0.0402490496635437, 'Neutral': 0.34519028663635254, 'Mixed': 0.10622867196798325}\n",
      "\n",
      "ASYNCHRONOUS RESULTS:\n",
      "Mixed              0.106229\n",
      "Negative           0.040249\n",
      "Neutral             0.34519\n",
      "Positive           0.508332\n",
      "File         walmart_1k.csv\n",
      "Sentiment          POSITIVE\n",
      "Name: 235, dtype: object\n"
     ]
    }
   ],
   "source": [
    "record_no = 235\n",
    "# Tweet text\n",
    "print('TWEET TEXT:\\n', \n",
    "      df.loc[record_no].item())\n",
    "# Real Time Results\n",
    "print('\\nREAL TIME RESULTS:\\n') \n",
    "print(comprehend.detect_sentiment(Text=df.loc[record_no].item(), LanguageCode='en')['SentimentScore'])\n",
    "# Job Resutls\n",
    "print('\\nASYNCHRONOUS RESULTS:')\n",
    "print(sentiment_results.loc[record_no])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = 'Comprehend/outputs/sentiment_results_walmart_1k.xlsx'\n",
    "sentiment_results['Text'] = df.walmart_tweets\n",
    "sentiment_results.to_excel(output_name, engine = 'xlsxwriter',  encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
